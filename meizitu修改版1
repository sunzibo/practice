import os
import shutil

import requests
from lxml import etree
import time
import traceback


class MZiTu:

    # 初始化对象属性
    def __init__(self):
        self.index_url = "https://www.mzitu.com/"
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.97 Safari/537.36",
            "Referer": "https://www.mzitu.com/"
        }
        self.path = input('请输入保存的文件夹名(默认为--D://1122--):')

    # 发送request请求
    def send_request(self, url):
        """获取url的二进制代码"""
        return requests.get(url, headers=self.headers, timeout=30).content  # .content以字节的方式访问请求响应体，对于非文本请求

    # 解析每页的数据
    def parse(self, html_str):  # parse [pɑrs]解析
        print("*" * 14, '解析每页数据', "*" * 14)
        html = etree.HTML(html_str)  # etree.HTML():构造了一个XPath解析对象并对HTML文本进行自动修正。
        titles = html.xpath('//img[@class="lazy"]')
        # 部分源码为 <img class="lazy xh-highlight" src="https://i.mmzztt.com/..."  alt="单眼皮..."
        content_list = []
        for title in titles:
            item = {}
            item['title'] = title.xpath('./@alt')[0]  # .选取当前节点    直接将名称以键值对形式写入字典中
            item['href'] = title.xpath('../@href')[0]  # ..选取当前节点的父节点
            content_list.append(item)  # 将每一个标题和链接写入字典后放入列表中，比如[ { },{ } ]
            # print(item)
        print('每页数据的解析结果：  ', content_list, '\n')
        next_url = html.xpath('//a[contains(text(),"下一页")]/@href')
        next_url = next_url[0] if next_url else None  # 取出下一页的链接，如果next_url=True,next_url = next_url[0] ,否则返回None
        return content_list, next_url

    # 获取每张写真集的img_url
    def get_img_url(self, detail_html):
        html = etree.HTML(detail_html)  # etree.HTML():构造了一个XPath解析对象并对HTML文本进行自动修正。

        try:  # 这是我自己加的，第一次运行这里出现了错误，之后运行未出现类似情况，属于个例
            img_url = html.xpath('//div[@class="main-image"]/p/a/img/@src')[0]  # 为什么会出现找不到的情况？
        except:
            print('获取img_url失败，img_url列表为空' + html.xpath('//div[@class="main-image"]/p/a/img/@src'))
            return

        next_img_url = html.xpath('//span[contains(text(),"下一页")]/../@href')
        next_url = next_img_url[0] if next_img_url else None  # 取出下一页的链接
        return img_url, next_url

    # 判断文件夹是否存在,不存在创建文件夹，存在则跳过
    def mkdir(self, dir_name):
        print("*" * 14, f'判断--{dir_name}--是否已存在', "*" * 14)
        # total_image = len(img_url_list)  # 图像总数= url的个数，可以显示有多少张图片
        meizi_dir = self.path if self.path else 'D://1122'  # 判断是否有指定目录，没有则默认'D://1122'
        final_dir = meizi_dir + '/' + dir_name  # 最终的文件夹路径
        if os.path.exists(final_dir):
            print("-" * 30 + '文件{}已存在'.format(dir_name) + "-" * 30, '\n')
            final_dir = None  # 如果文件已存在，则跳过，不进行删除再下载
            # shutil.rmtree(final_dir)   # 递归地删除文件
        else:
            print("-" * 30 + '文件{}未存在，已创建'.format(dir_name) + "-" * 30, '\n')
            os.makedirs(final_dir)
        return final_dir

    # 保存img
    def save_image(self, dir_name, final_dir, img_url_list):
        print("*" * 14, f'保存--{dir_name}--图片', "*" * 14)
        for img_url in img_url_list:  # 遍历每一个照片的链接
            try:  # 这个try用的很好，就在请求链接时使用，如果连接失败则跳过后面进入下一个循环
                image_data = self.send_request(img_url)
            except:
                continue
            file_name = final_dir + '/' + img_url[-9:]  # 最终文件名
            with open(file_name, 'wb') as image:
                image.write(image_data)  # 保存照片
            print("*" * 14, img_url, '下载完成', "*" * 14)  # 每一张保存后显示结果
        print("-" * 29 + '写真集{}保存完毕'.format(dir_name) + "-" * 30 + '\n\n')  # 一套保存后显示结果

    # 运行爬虫
    def run(self):
        # 1. 获取url
        next_page_url = self.index_url
        i = 1
        # 获取每页的url地址并解析
        while True:
            # 2. 发送请求,获取响应
            try:
                html_str = self.send_request(next_page_url).decode()  # 将二进制的数据解码
            except:
                continue
            # 3. 解析数据
            content_list, next_page_url = self.parse(html_str)  # 获取每页的图集信息和下一页情况
            # 4. 获取详情页的img
            j = 1
            # 获取每张写真集并解析
            for content in content_list:  # 将每一个写真集的数据从列表中取出
                img_url_list = []
                # 获取每张写真集的img_url
                # 第一页的img地址
                dir_name = content['title']  # 将字典中的数据取出，写真集的名称
                next_url = content['href']  # 将字典中的数据取出，写真集的链接
                print("-" * 30 + '正在获取写真集{}'.format(dir_name) + "-" * 30)
                print('写真集的链接  ', next_url, '\n')
                time.sleep(0.5)

                # 找出符合条件的进行下载
                if '丝' not in dir_name:
                    continue
                else:
                    print('wa~~~~~~~o', dir_name, '来喽！')

                # 先判断文件是否存在，在进行遍历信息
                final_dir = self.mkdir(dir_name)  # 最终的保存路径
                if final_dir is None:
                    continue  # 文件存在时跳过

                # 获取每张写真集每页的img_url
                while True:  # 这个循环不太理解，为什么每次都要获取写真集的源代码
                    try:
                        detail_html = self.send_request(next_url).decode()
                        # 因为这个网址一个网页只显示一张图片，所以next_url是写真集的链接，确切的说是写真集第一张图片的链接，要获得其他的图片就需要get_img_url()产生下一张图片的链接
                    except:
                        continue
                    try:
                        img_url, next_url = self.get_img_url(detail_html)
                        # 获取img_url为本页网页内需要的图片的链接，next_url为下一页网页的链接
                    except:
                        continue
                    # 第二页的img地址
                    # detail_html = self.send_request(next_url)
                    # img_url, next_url =self.get_img_url(detail_html)
                    img_url_list.append(img_url)
                    time.sleep(0.5)
                    if next_url is None:  # 直到下一页为空时，打破循环
                        break
                # 保存图片
                if img_url_list:
                    self.save_image(dir_name, final_dir, img_url_list)  # 保存图片
                    time.sleep(0.5)
                j += 1
            print("-" * 32 + '第{}页获取完成'.format(int(i)) + "-" * 32 + '\n\n')
            i += 1
            if next_page_url is None:  # 当下一页的url为空时打破循环
                break


def main():
    mz = MZiTu()
    mz.run()


if __name__ == '__main__':
    main()
