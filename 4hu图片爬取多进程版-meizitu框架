#  里面的注释都是meizitu框架里的，没有修改，不具有参考价值

import os
import shutil

import requests
from lxml import etree
import time
import multiprocessing


class MZiTu:

    # 初始化对象属性
    def __init__(self):
        self.index_url = "https://www.3567yy.com/pic/html28/"
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.97 Safari/537.36",
            "Referer": "https://www.3567yy.com/pic/html28"
        }
        self.path = input('请输入保存的文件夹名(默认为--D://1122--):')

    # 发送request请求
    def send_request(self, url):
        """获取url的二进制代码"""
        return requests.get(url, headers=self.headers, timeout=30).content  # .content以字节的方式访问请求响应体，对于非文本请求

    # 解析每页的数据
    def parse(self, html_str):  # parse [pɑrs]解析
        print("*" * 14, '解析每页数据', "*" * 14)
        html = etree.HTML(html_str)  # etree.HTML():构造了一个XPath解析对象并对HTML文本进行自动修正。
        titles = html.xpath('//a[@class="video-pic loading"]')
        # 部分源码为 <img class="lazy xh-highlight" src="https://i.mmzztt.com/..."  alt="单眼皮..."
        content_list = []
        for title in titles:
            item = {}
            item['title'] = title.xpath('./@title')[0]
            item['href'] = title.xpath('./@href')[0]  # 需增加https://www.3567yy.com
            content_list.append(item)  # 将每一个标题和链接写入字典后放入列表中，比如[ { },{ } ]
            # print(item)
        print('每页数据的解析结果：  ', content_list, '\n')

        return content_list

    # 获取每张写真集的img_url
    def get_img_url(self, detail_html):
        html = etree.HTML(detail_html)  # etree.HTML():构造了一个XPath解析对象并对HTML文本进行自动修正。
        img_url = html.xpath('//div[@class="details-content text-justify"]/img/@src')

        return img_url

    # 判断文件夹是否存在,不存在创建文件夹，存在则跳过
    def mkdir(self, dir_name):
        print("*" * 14, f'判断--{dir_name}--是否已存在', "*" * 14)
        # total_image = len(img_url_list)  # 图像总数= url的个数，可以显示有多少张图片
        meizi_dir = self.path if self.path else 'D://1122'  # 判断是否有指定目录，没有则默认'D://1122'
        final_dir = meizi_dir + '/' + dir_name  # 最终的文件夹路径
        if os.path.exists(final_dir):
            print("-" * 30 + '文件{}已存在'.format(dir_name) + "-" * 30, '\n')
            final_dir = None  # 如果文件已存在，则跳过，不进行删除再下载
            # shutil.rmtree(final_dir)   # 递归地删除文件
        else:
            print("-" * 30 + '文件{}未存在，已创建'.format(dir_name) + "-" * 30, '\n')
            os.makedirs(final_dir)
        return final_dir

    # 保存img
    def save_image(self, dir_name, final_dir, img_url_list):
        print("*" * 14, f'保存--{dir_name}--图片', "*" * 14)
        for img_url in img_url_list:  # 遍历每一个照片的链接
            try:  # 这个try用的很好，就在请求链接时使用，如果连接失败则跳过后面进入下一个循环
                image_data = self.send_request(img_url)
            except:
                continue
            file_name = final_dir + '/' + img_url[-9:]  # 最终文件名
            with open(file_name, 'wb') as image:
                image.write(image_data)  # 保存照片
            print("*" * 14, img_url, '下载完成', "*" * 14)  # 每一张保存后显示结果
        print("-" * 29 + '写真集{}保存完毕'.format(dir_name) + "-" * 30 + '\n\n')  # 一套保存后显示结果

    # 运行爬虫
    def run(self, i):
        # 1. 获取url

        # 获取每页的url地址并解析

        page_url = self.index_url + f'index_{i}.html'
        # 2. 发送请求,获取响应
        try:
            html_str = self.send_request(page_url).decode()  # 将二进制的数据解码
        except:
            return
        # 3. 解析数据
        content_list = self.parse(html_str)  # 获取每页的图集信息
        # 4. 获取详情页的img
        # 获取每张写真集并解析
        for content in content_list:  # 将每一个写真集的数据从列表中取出
            # 获取每张写真集的img_url
            # 第一页的img地址
            dir_name = content['title']  # 将字典中的数据取出，写真集的名称
            pic_href = content['href']  # 将字典中的数据取出，写真集的链接
            pic_url = 'https://www.3567yy.com' + pic_href
            print("-" * 30 + '正在获取写真集{}'.format(dir_name) + "-" * 30)
            print('写真集的链接  ', pic_url, '\n')
            time.sleep(0.5)

            # 找出符合条件的进行下载
            if '丝' not in dir_name:
                continue
            else:
                print('wa~~~~~~~o', dir_name, '来喽！')

            # 先判断文件是否存在，在进行遍历信息
            final_dir = self.mkdir(dir_name)  # 最终的保存路径
            if final_dir is None:
                continue  # 文件存在时跳过

            # 获取每张写真集每页的img_url
            try:
                detail_html = self.send_request(pic_url).decode()
                # 因为这个网址一个网页只显示一张图片，所以next_url是写真集的链接，确切的说是写真集第一张图片的链接，要获得其他的图片就需要get_img_url()产生下一张图片的链接
            except:
                continue
            try:
                img_url_list = self.get_img_url(detail_html)
                # 获取img_url为本页网页内需要的图片的链接，next_url为下一页网页的链接
            except:
                continue

            # 保存图片
            if img_url_list:
                self.save_image(dir_name, final_dir, img_url_list)  # 保存图片
                time.sleep(0.5)
        print("-" * 32 + '第{}页获取完成'.format(int(i)) + "-" * 32 + '\n\n')




def main():
    mz = MZiTu()
    pool_info = multiprocessing.Pool(processes=5)
    for i in range(1, 20):
        pool_info.apply_async(mz.run, (i, ))
        # mz.run(i)
    pool_info.close()
    pool_info.join()


if __name__ == '__main__':
    main()
